{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fe5e4d",
   "metadata": {},
   "source": [
    "# Aldar K√∂se Storyboard Generator - Google Colab Setup\n",
    "\n",
    "This notebook runs the image generation backend on Google Colab with GPU acceleration.\n",
    "\n",
    "**Two modes:**\n",
    "1. **Full Server Mode**: Run the entire Flask app in Colab (accessible via ngrok)\n",
    "2. **API Mode**: Expose only the generation API for your local Flask app to call\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- Your code repository accessible (GitHub or Drive)\n",
    "- ngrok account (free) for public URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8443405",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66230cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GPU Available - Ready for fast SDXL generation!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q flask python-dotenv pillow requests openai\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q diffusers transformers accelerate safetensors peft scipy omegaconf\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a17d7",
   "metadata": {},
   "source": [
    "## 2. Clone Your Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44595b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone from GitHub\n",
    "!git clone https://github.com/sapogeth/qylysh-higgsfiled.git\n",
    "%cd qylysh-higgsfiled\n",
    "\n",
    "# Option B: Upload files manually\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your project zip\n",
    "# !unzip -q qylysh-higgsfiled.zip\n",
    "# %cd qylysh-higgsfiled\n",
    "\n",
    "print(\"‚úì Repository loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110104d8",
   "metadata": {},
   "source": [
    "## 3. Configure for Colab (Update config for CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file with your OpenAI API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Optional: For GPT-based story generation\n",
    "api_key = getpass(\"Enter your OpenAI API key (or press Enter to skip): \")\n",
    "if api_key:\n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(f'OPENAI_API_KEY={api_key}\\n')\n",
    "    print(\"‚úì API key saved\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No API key - will use fallback story generation\")\n",
    "\n",
    "# Update config.py for CUDA device\n",
    "import torch\n",
    "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4d4bc",
   "metadata": {},
   "source": [
    "## 4A. Mode 1: Run Full Flask Server with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ngrok auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "from pyngrok import ngrok, conf\n",
    "from getpass import getpass\n",
    "\n",
    "ngrok_token = getpass(\"Enter your ngrok auth token: \")\n",
    "conf.get_default().auth_token = ngrok_token\n",
    "\n",
    "print(\"‚úì ngrok configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Colab-optimized app runner\n",
    "with open('app_colab.py', 'w') as f:\n",
    "    f.write('''\n",
    "# Colab-optimized version of app.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Override config for CUDA before imports\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Patch config.py for CUDA\n",
    "import config\n",
    "config.DEVICE = \"cuda\"\n",
    "config.DTYPE = \"float16\"\n",
    "config.LAZY_LOAD_MODEL = False  # Preload in Colab\n",
    "config.ENABLE_TORCH_COMPILE = False  # Not needed in Colab\n",
    "\n",
    "# Import the main app\n",
    "from app import app, generator\n",
    "\n",
    "# Force local mode in Colab\n",
    "generator.use_local = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=\"*70)\n",
    "    print(\"COLAB MODE: Using CUDA GPU for SDXL generation\")\n",
    "    print(\"=\"*70)\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "''')\n",
    "\n",
    "print(\"‚úì Colab runner created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ae4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Flask server with ngrok tunnel\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(5000)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üåê PUBLIC URL (share this):\")\n",
    "print(f\"   {public_url}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server\")\n",
    "print(\"   Open the URL above in your browser to use the app\\n\")\n",
    "\n",
    "# Run Flask in background thread\n",
    "def run_flask():\n",
    "    os.system('python app_colab.py')\n",
    "\n",
    "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "\n",
    "# Keep cell running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nServer stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9770a13",
   "metadata": {},
   "source": [
    "## 4B. Mode 2: API-Only Mode (for Local Flask App)\n",
    "\n",
    "Use this if you want to keep your Flask UI running locally but offload generation to Colab GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75419bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lightweight API server for remote generation\n",
    "with open('colab_api.py', 'w') as f:\n",
    "    f.write('''\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Configure for CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import config\n",
    "config.DEVICE = \"cuda\"\n",
    "config.DTYPE = \"float16\"\n",
    "\n",
    "from local_image_generator import LocalImageGenerator\n",
    "from prompt_enhancer import PromptEnhancer\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Allow cross-origin requests from local app\n",
    "\n",
    "# Initialize generator\n",
    "print(\"Loading SDXL model on GPU...\")\n",
    "generator = LocalImageGenerator(lazy_load=False)\n",
    "enhancer = PromptEnhancer()\n",
    "print(\"‚úì Model ready\")\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'device': config.DEVICE,\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'none'\n",
    "    })\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    \"\"\"Generate a single image from prompt\"\"\"\n",
    "    data = request.get_json()\n",
    "    prompt = data.get('prompt', '')\n",
    "    negative_prompt = data.get('negative_prompt', config.NEGATIVE_PROMPT)\n",
    "    \n",
    "    # Optional IP-Adapter support\n",
    "    ref_image_base64 = data.get('ref_image_base64')\n",
    "    ip_adapter_scale = data.get('ip_adapter_scale', 0.6)\n",
    "    \n",
    "    ref_image = None\n",
    "    if ref_image_base64:\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "        from PIL import Image\n",
    "        img_data = base64.b64decode(ref_image_base64)\n",
    "        ref_image = Image.open(BytesIO(img_data)).convert('RGB')\n",
    "    \n",
    "    # Generate image\n",
    "    image = generator.generate_single(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        ref_image=ref_image,\n",
    "        ip_adapter_scale=ip_adapter_scale if ref_image else None\n",
    "    )\n",
    "    \n",
    "    # Convert to base64\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    \n",
    "    return jsonify({\n",
    "        'success': True,\n",
    "        'image_base64': img_base64\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "''')\n",
    "\n",
    "# Install CORS support\n",
    "!pip install -q flask-cors\n",
    "\n",
    "print(\"‚úì API server created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d571dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start API server with ngrok\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(5000)\n",
    "api_url = f\"{public_url}/generate\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üåê COLAB API ENDPOINT:\")\n",
    "print(f\"   {api_url}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAdd this to your local .env file:\")\n",
    "print(f\"   COLAB_API_URL={public_url}\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the API\")\n",
    "\n",
    "# Run Flask in background\n",
    "def run_api():\n",
    "    os.system('python colab_api.py')\n",
    "\n",
    "api_thread = threading.Thread(target=run_api, daemon=True)\n",
    "api_thread.start()\n",
    "\n",
    "# Keep running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nAPI stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aaee3e",
   "metadata": {},
   "source": [
    "## 5. Test the Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e22e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of local generation\n",
    "import torch\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "from local_image_generator import LocalImageGenerator\n",
    "gen = LocalImageGenerator(lazy_load=False)\n",
    "\n",
    "test_prompt = \"Aldar Kose walking across the golden steppe, 2D illustration\"\n",
    "print(f\"\\nGenerating test image...\")\n",
    "\n",
    "img = gen.generate_single(test_prompt)\n",
    "img.save('test_output.png')\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image('test_output.png'))\n",
    "print(\"\\n‚úì Generation successful!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
