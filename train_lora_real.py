"""
–†–µ–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LoRA –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ê–ª–¥–∞—Ä –ö–æ—Å–µ
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PEFT (Parameter-Efficient Fine-Tuning) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
"""

import torch
import torch.nn.functional as F
from diffusers import StableDiffusionXLPipeline, DDPMScheduler, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from PIL import Image
import numpy as np
from pathlib import Path
from tqdm import tqdm
import json
import config
from feature_analyzer import FeatureAnalyzer


class LoRATrainer:
    """
    –†–µ–∞–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä LoRA –¥–ª—è Stable Diffusion XL
    """

    def __init__(self, reference_images: list, output_path: Path):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞

        Args:
            reference_images: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π LoRA
        """
        self.reference_images = reference_images
        self.output_path = output_path
        self.device = config.get_device()
        self.dtype = config.get_dtype()

        print("=" * 70)
        print("–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ê–õ–¨–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø LORA")
        print("=" * 70)
        print()

    def load_base_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π SDXL –º–æ–¥–µ–ª–∏"""
        print("[1/5] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π SDXL –º–æ–¥–µ–ª–∏...")

        self.pipe = StableDiffusionXLPipeline.from_pretrained(
            config.SDXL_MODEL_ID,
            torch_dtype=self.dtype,
            use_safetensors=True,
            variant="fp16" if self.dtype == torch.float16 else None
        )

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.pipe = self.pipe.to(self.device)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è M1
        if config.ENABLE_ATTENTION_SLICING:
            self.pipe.enable_attention_slicing()
        if config.ENABLE_VAE_SLICING:
            self.pipe.enable_vae_slicing()

        print("‚úì SDXL –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        print()

    def analyze_references(self):
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞"""
        print("[2/5] –ê–Ω–∞–ª–∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")

        analyzer = FeatureAnalyzer()
        aggregated = analyzer.analyze_reference_images(self.reference_images)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
        self.training_prompt = analyzer.generate_training_prompt(aggregated)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
        analysis_path = config.MODELS_DIR / "training_analysis.json"
        analyzer.save_analysis(aggregated, analysis_path)

        print(f"‚úì –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {self.training_prompt}")
        print()

        return self.training_prompt

    def setup_lora(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        print("[3/5] –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞...")

        # LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è UNet
        lora_config = LoraConfig(
            r=config.LORA_RANK,  # 16
            lora_alpha=config.LORA_ALPHA,  # 16
            target_modules=[
                "to_q", "to_k", "to_v", "to_out.0",
                "proj_in", "proj_out",
                "ff.net.0.proj", "ff.net.2"
            ],
            lora_dropout=0.1,
            bias="none",
        )

        # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ UNet
        self.pipe.unet = get_peft_model(self.pipe.unet, lora_config)

        print(f"‚úì LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ (rank={config.LORA_RANK}, alpha={config.LORA_ALPHA})")
        print(f"‚úì –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.pipe.unet.get_nb_trainable_parameters()}")
        print()

    def prepare_training_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("[4/5] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        self.images = []
        self.prompts = []

        for img_path in self.reference_images:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img = Image.open(img_path).convert('RGB')
            img = img.resize((config.IMAGE_WIDTH, config.IMAGE_HEIGHT), Image.Resampling.LANCZOS)

            self.images.append(img)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            self.prompts.append(self.training_prompt)

        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        print()

    def train(self, num_epochs: int = 100, learning_rate: float = 1e-4):
        """
        –û–±—É—á–µ–Ω–∏–µ LoRA

        Args:
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            learning_rate: Learning rate
        """
        print("[5/5] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è LoRA...")
        print(f"  –≠–ø–æ—Ö–∏: {num_epochs}")
        print(f"  Learning rate: {learning_rate}")
        print(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print()

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—Ç–æ–ª—å–∫–æ –¥–ª—è LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        optimizer = torch.optim.AdamW(
            self.pipe.unet.parameters(),
            lr=learning_rate,
            betas=(0.9, 0.999),
            weight_decay=1e-2
        )

        # Scheduler –¥–ª—è noise
        noise_scheduler = DDPMScheduler.from_pretrained(
            config.SDXL_MODEL_ID,
            subfolder="scheduler"
        )

        # Training loop
        self.pipe.unet.train()

        progress_bar = tqdm(range(num_epochs), desc="–û–±—É—á–µ–Ω–∏–µ")

        for epoch in progress_bar:
            total_loss = 0

            for img, prompt in zip(self.images, self.prompts):
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ latent
                with torch.no_grad():
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    img_tensor = torch.from_numpy(np.array(img)).float() / 255.0
                    img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0)  # [1, 3, H, W]
                    img_tensor = img_tensor.to(self.device, dtype=self.dtype)

                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è VAE
                    img_tensor = (img_tensor - 0.5) * 2.0

                    # Encode –≤ latent space
                    latents = self.pipe.vae.encode(img_tensor).latent_dist.sample()
                    latents = latents * self.pipe.vae.config.scaling_factor

                # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
                noise = torch.randn_like(latents)
                timesteps = torch.randint(
                    0, noise_scheduler.config.num_train_timesteps,
                    (1,), device=self.device
                ).long()

                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # –ü–æ–ª—É—á–∞–µ–º text embeddings
                with torch.no_grad():
                    text_inputs = self.pipe.tokenizer(
                        prompt,
                        padding="max_length",
                        max_length=self.pipe.tokenizer.model_max_length,
                        truncation=True,
                        return_tensors="pt"
                    )
                    text_embeddings = self.pipe.text_encoder(
                        text_inputs.input_ids.to(self.device)
                    )[0]

                    # –î–ª—è SDXL –Ω—É–∂–µ–Ω –≤—Ç–æ—Ä–æ–π text encoder
                    text_inputs_2 = self.pipe.tokenizer_2(
                        prompt,
                        padding="max_length",
                        max_length=self.pipe.tokenizer_2.model_max_length,
                        truncation=True,
                        return_tensors="pt"
                    )
                    text_embeddings_2 = self.pipe.text_encoder_2(
                        text_inputs_2.input_ids.to(self.device)
                    )[0]

                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º embeddings
                    pooled_prompt_embeds = text_embeddings_2
                    prompt_embeds = torch.cat([text_embeddings, text_embeddings_2], dim=-1)

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ noise
                model_pred = self.pipe.unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": torch.zeros((1, 6), device=self.device, dtype=self.dtype)
                    }
                ).sample

                # Loss
                loss = F.mse_loss(model_pred, noise, reduction="mean")

                # Backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(self.images)
            progress_bar.set_postfix({"loss": f"{avg_loss:.4f}"})

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∫–∞–∂–¥—ã–µ 20 —ç–ø–æ—Ö
            if (epoch + 1) % 20 == 0:
                self.save_checkpoint(epoch + 1, avg_loss)

        print()
        print("‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print()

    def save_checkpoint(self, epoch: int, loss: float):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ LoRA"""
        checkpoint_path = self.output_path.parent / f"lora_checkpoint_epoch{epoch}.safetensors"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ LoRA –≤–µ—Å–∞
        self.pipe.unet.save_pretrained(checkpoint_path)

        print(f"  üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_path} (epoch {epoch}, loss: {loss:.4f})")

    def save_final_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π LoRA –º–æ–¥–µ–ª–∏...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –≤–µ—Å–∞
        self.pipe.unet.save_pretrained(self.output_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "model_type": "LoRA for SDXL",
            "base_model": config.SDXL_MODEL_ID,
            "training_prompt": self.training_prompt,
            "trigger_word": config.LORA_TRIGGER_WORD,
            "lora_rank": config.LORA_RANK,
            "lora_alpha": config.LORA_ALPHA,
            "num_reference_images": len(self.reference_images),
            "reference_images": [str(p.name) for p in self.reference_images]
        }

        metadata_path = self.output_path.parent / "lora_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        print(f"‚úì LoRA –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.output_path}")
        print(f"‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
        print()

    def generate_test_image(self, prompt: str) -> Image.Image:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–±—É—á–µ–Ω–Ω–æ–π LoRA"""
        print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")

        self.pipe.unet.eval()

        with torch.no_grad():
            image = self.pipe(
                prompt=prompt,
                negative_prompt=config.NEGATIVE_PROMPT,
                num_inference_steps=config.NUM_INFERENCE_STEPS,
                guidance_scale=config.GUIDANCE_SCALE,
                height=config.IMAGE_HEIGHT,
                width=config.IMAGE_WIDTH
            ).images[0]

        test_image_path = config.OUTPUT_DIR / "lora_test_generation.png"
        image.save(test_image_path)

        print(f"‚úì –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {test_image_path}")
        print()

        return image


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""

    print("=" * 70)
    print("–†–ï–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï LORA –î–õ–Ø –ê–õ–î–ê–† –ö–û–°–ï")
    print("=" * 70)
    print()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = LoRATrainer(
        reference_images=config.REFERENCE_IMAGES,
        output_path=config.LORA_PATH
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    trainer.load_base_model()

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤
    training_prompt = trainer.analyze_references()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    trainer.setup_lora()

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    trainer.prepare_training_data()

    # –û–±—É—á–µ–Ω–∏–µ
    # –í–ê–ñ–ù–û: –¥–ª—è M1 MacBook Air —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–µ–Ω—å—à–µ —ç–ø–æ—Ö (100-200)
    # –î–ª—è –±–æ–ª–µ–µ –º–æ—â–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 500-1000
    trainer.train(
        num_epochs=100,  # –ù–∞—á–Ω–∏—Ç–µ —Å –º–∞–ª–æ–≥–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        learning_rate=1e-4
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    trainer.save_final_model()

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    test_prompt = f"{config.LORA_TRIGGER_WORD}, {training_prompt}"
    trainer.generate_test_image(test_prompt)

    print("=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 70)
    print()
    print("–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ static/generated/lora_test_generation.png")
    print("  2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: python3 train_lora_competitive.py")
    print("  3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é LoRA –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏: python3 app.py")
    print()


if __name__ == "__main__":
    main()
