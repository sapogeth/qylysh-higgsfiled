"""
Image Comparator - –°–∏—Å—Ç–µ–º–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç LoRA –∏ –¥—Ä—É–≥–∏—Ö AI –º–æ–¥–µ–ª–µ–π —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º–∏
"""

import torch
from PIL import Image
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import json
from transformers import CLIPProcessor, CLIPModel
from skimage.metrics import structural_similarity as ssim
from scipy.spatial.distance import cosine
import config


class ImageComparator:
    """
    –°–∏—Å—Ç–µ–º–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ:
    1. CLIP embeddings similarity (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
    2. Structural similarity (—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
    3. Feature matching (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º)
    """

    def __init__(self, reference_images: List[Path], key_features: List[str]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä–∞

        Args:
            reference_images: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            key_features: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–∏–∑ FeatureAnalyzer)
        """
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Image Comparator...")

        self.reference_images = reference_images
        self.key_features = key_features

        # –ó–∞–≥—Ä—É–∑–∫–∞ CLIP –º–æ–¥–µ–ª–∏
        self.device = config.get_device()
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.model = self.model.to(self.device)
        self.model.eval()

        # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ embeddings —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ embeddings —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        self.reference_embeddings = self._compute_reference_embeddings()

        # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ embeddings –∫–ª—é—á–µ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ embeddings –∫–ª—é—á–µ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫...")
        self.feature_embeddings = self._compute_feature_embeddings()

        print("‚úì Image Comparator –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

    def _compute_reference_embeddings(self) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ CLIP embeddings –¥–ª—è –≤—Å–µ—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤"""
        embeddings = []

        for img_path in self.reference_images:
            if img_path.exists():
                image = Image.open(img_path).convert('RGB')
                embedding = self._get_image_embedding(image)
                embeddings.append(embedding)

        return torch.stack(embeddings)

    def _compute_feature_embeddings(self) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ CLIP text embeddings –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        inputs = self.processor(
            text=self.key_features,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            text_embeddings = self.model.get_text_features(**inputs)
            text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

        return text_embeddings

    def _get_image_embedding(self, image: Image.Image) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ CLIP embedding –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        inputs = self.processor(
            images=image,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            image_embedding = self.model.get_image_features(**inputs)
            image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)

        return image_embedding.squeeze(0)

    def compare_with_references(
        self,
        generated_image: Image.Image,
        provider_name: str = "unknown"
    ) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º–∏

        Args:
            generated_image: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            provider_name: –ò–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (lora, freepik, deepai, etc.)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        print(f"\n[{provider_name.upper()}] –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º–∏...")

        # 1. CLIP Similarity —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º–∏
        gen_embedding = self._get_image_embedding(generated_image)

        clip_similarities = []
        for ref_emb in self.reference_embeddings:
            similarity = torch.cosine_similarity(
                gen_embedding.unsqueeze(0),
                ref_emb.unsqueeze(0)
            ).item()
            clip_similarities.append(similarity)

        avg_clip_similarity = float(np.mean(clip_similarities))
        max_clip_similarity = float(np.max(clip_similarities))

        print(f"  CLIP similarity: avg={avg_clip_similarity:.3f}, max={max_clip_similarity:.3f}")

        # 2. Feature Matching Score
        feature_score = self._calculate_feature_matching(gen_embedding)
        print(f"  Feature matching: {feature_score:.3f}")

        # 3. Structural Similarity (—Å–æ –≤—Å–µ–º–∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º–∏)
        ssim_scores = []
        for img_path in self.reference_images:
            if img_path.exists():
                ref_image = Image.open(img_path).convert('RGB')
                ssim_score = self._calculate_ssim(generated_image, ref_image)
                ssim_scores.append(ssim_score)

        avg_ssim = float(np.mean(ssim_scores)) if ssim_scores else 0.0
        print(f"  Structural similarity (SSIM): {avg_ssim:.3f}")

        # 4. –û–±—â–∏–π Quality Score (weighted average)
        quality_score = (
            0.4 * avg_clip_similarity +
            0.4 * feature_score +
            0.2 * avg_ssim
        )

        print(f"  üìä –û–±—â–∏–π Quality Score: {quality_score:.3f}")

        return {
            "provider": provider_name,
            "clip_similarity_avg": avg_clip_similarity,
            "clip_similarity_max": max_clip_similarity,
            "feature_matching_score": feature_score,
            "ssim_avg": avg_ssim,
            "quality_score": quality_score,
            "individual_clip_scores": clip_similarities,
            "individual_ssim_scores": ssim_scores
        }

    def _calculate_feature_matching(self, image_embedding: torch.Tensor) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ score —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–ª—é—á–µ–≤—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º

        Args:
            image_embedding: CLIP embedding –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        Returns:
            Feature matching score (0-1)
        """
        # –í—ã—á–∏—Å–ª—è–µ–º cosine similarity –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ –∫–∞–∂–¥–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–æ–π
        similarities = []

        for feature_emb in self.feature_embeddings:
            similarity = torch.cosine_similarity(
                image_embedding.unsqueeze(0),
                feature_emb.unsqueeze(0)
            ).item()
            similarities.append(similarity)

        # –°—Ä–µ–¥–Ω–∏–π score –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
        avg_score = float(np.mean(similarities))

        return avg_score

    def _calculate_ssim(
        self,
        img1: Image.Image,
        img2: Image.Image
    ) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Structural Similarity Index

        Args:
            img1: –ü–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img2: –í—Ç–æ—Ä–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ

        Returns:
            SSIM score (0-1)
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        size = (256, 256)  # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        img1_resized = img1.resize(size, Image.Resampling.LANCZOS)
        img2_resized = img2.resize(size, Image.Resampling.LANCZOS)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ grayscale numpy arrays
        img1_gray = np.array(img1_resized.convert('L'))
        img2_gray = np.array(img2_resized.convert('L'))

        # –í—ã—á–∏—Å–ª—è–µ–º SSIM
        score = ssim(img1_gray, img2_gray)

        return float(score)

    def compare_providers(
        self,
        provider_results: Dict[str, Image.Image]
    ) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

        Args:
            provider_results: –°–ª–æ–≤–∞—Ä—å {provider_name: generated_image}

        Returns:
            –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        """
        print("\n" + "=" * 70)
        print("–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –û–¢ –í–°–ï–• –ü–†–û–í–ê–ô–î–ï–†–û–í")
        print("=" * 70)

        comparisons = {}

        for provider, image in provider_results.items():
            if image is not None:
                comparison = self.compare_with_references(image, provider)
                comparisons[provider] = comparison

        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ quality score
        ranked = sorted(
            comparisons.items(),
            key=lambda x: x[1]['quality_score'],
            reverse=True
        )

        print("\n" + "=" * 70)
        print("–ò–¢–û–ì–û–í–´–ô –†–ï–ô–¢–ò–ù–ì")
        print("=" * 70)

        for rank, (provider, metrics) in enumerate(ranked, 1):
            print(f"{rank}. {provider.upper()}: {metrics['quality_score']:.3f}")

        return {
            "comparisons": comparisons,
            "ranking": [(provider, metrics['quality_score']) for provider, metrics in ranked],
            "best_provider": ranked[0][0] if ranked else None,
            "best_score": ranked[0][1]['quality_score'] if ranked else 0.0
        }

    def generate_feedback_for_lora(
        self,
        lora_result: Dict[str, Any],
        best_competitor: Dict[str, Any],
        competitor_name: str
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LoRA

        Args:
            lora_result: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã LoRA
            best_competitor: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª—É—á—à–µ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
            competitor_name: –ò–º—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞

        Returns:
            Feedback —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        print("\n" + "=" * 70)
        print("–ê–ù–ê–õ–ò–ó –ò –û–ë–†–ê–¢–ù–ê–Ø –°–í–Ø–ó–¨ –î–õ–Ø LORA")
        print("=" * 70)

        feedback = {
            "lora_score": lora_result['quality_score'],
            "best_competitor": competitor_name,
            "competitor_score": best_competitor['quality_score'],
            "score_gap": best_competitor['quality_score'] - lora_result['quality_score'],
            "needs_improvement": best_competitor['quality_score'] > lora_result['quality_score'],
            "recommendations": []
        }

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–Ω–∏—Ü—É –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
        print(f"\nLoRA Score: {lora_result['quality_score']:.3f}")
        print(f"–õ—É—á—à–∏–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç ({competitor_name}): {best_competitor['quality_score']:.3f}")
        print(f"–†–∞–∑–Ω–∏—Ü–∞: {feedback['score_gap']:.3f}")

        if feedback['needs_improvement']:
            print("\n‚ö†Ô∏è  LoRA –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ —É–ª—É—á—à–µ–Ω–∏–∏!")
            print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
            if best_competitor['clip_similarity_avg'] > lora_result['clip_similarity_avg']:
                gap = best_competitor['clip_similarity_avg'] - lora_result['clip_similarity_avg']
                rec = f"–£–ª—É—á—à–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º–∏ (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {gap:.3f})"
                feedback['recommendations'].append({
                    "area": "clip_similarity",
                    "message": rec,
                    "priority": "high" if gap > 0.1 else "medium"
                })
                print(f"  ‚Ä¢ {rec}")

            if best_competitor['feature_matching_score'] > lora_result['feature_matching_score']:
                gap = best_competitor['feature_matching_score'] - lora_result['feature_matching_score']
                rec = f"–£–ª—É—á—à–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {gap:.3f})"
                feedback['recommendations'].append({
                    "area": "feature_matching",
                    "message": rec,
                    "priority": "high" if gap > 0.1 else "medium"
                })
                print(f"  ‚Ä¢ {rec}")

            if best_competitor['ssim_avg'] > lora_result['ssim_avg']:
                gap = best_competitor['ssim_avg'] - lora_result['ssim_avg']
                rec = f"–£–ª—É—á—à–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {gap:.3f})"
                feedback['recommendations'].append({
                    "area": "structural_similarity",
                    "message": rec,
                    "priority": "medium" if gap > 0.05 else "low"
                })
                print(f"  ‚Ä¢ {rec}")

            # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if feedback['score_gap'] > 0.15:
                feedback['recommendations'].append({
                    "area": "training",
                    "message": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å learning rate",
                    "priority": "high"
                })
                print("  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è")

        else:
            print("\n‚úì LoRA –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!")

        return feedback


def test_comparator():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä–∞"""

    print("=" * 70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï IMAGE COMPARATOR")
    print("=" * 70)
    print()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    feature_analysis_path = config.MODELS_DIR / "aldar_feature_analysis.json"

    if feature_analysis_path.exists():
        with open(feature_analysis_path, 'r', encoding='utf-8') as f:
            aggregated_features = json.load(f)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        key_features = []
        for category, info in aggregated_features.items():
            if info['avg_confidence'] > 0.2:
                key_features.append(info['feature'])
    else:
        print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ feature_analyzer.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        key_features = [
            "round friendly face",
            "narrow eyes",
            "mustache",
            "orange robe",
            "2D illustration"
        ]

    print(f"–ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {key_features}")
    print()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä–∞
    comparator = ImageComparator(config.REFERENCE_IMAGES, key_features)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –æ–¥–Ω–æ–º –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤
    test_image = Image.open(config.REFERENCE_IMAGES[0])

    result = comparator.compare_with_references(test_image, "test")

    print("\n" + "=" * 70)
    print("–¢–ï–°–¢ –ó–ê–í–ï–†–®–Å–ù")
    print("=" * 70)


if __name__ == "__main__":
    test_comparator()
