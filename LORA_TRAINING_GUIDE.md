# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –û–±—É—á–µ–Ω–∏—é LoRA –¥–ª—è –ê–ª–¥–∞—Ä –ö–æ—Å–µ

## –ß—Ç–æ —Ç–∞–∫–æ–µ LoRA?

**LoRA (Low-Rank Adaptation)** - —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞ machine learning –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.

```
–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (SDXL)     LoRA –∞–¥–∞–ø—Ç–µ—Ä          –†–µ–∑—É–ª—å—Ç–∞—Ç
     7 GB                   ~100 MB         –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    ‚Üì                          ‚Üì                      ‚Üì
[–û–±—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è]  +  [–í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ]  =  [–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ê–ª–¥–∞—Ä –ö–æ—Å–µ]
```

## –ó–∞—á–µ–º –æ–±—É—á–∞—Ç—å LoRA?

### –ë–µ–∑ LoRA (—Ç–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç—ã):
```
–ü—Ä–æ–º–ø—Ç: "Kazakh folk hero in orange robe"
–†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ—Ö–æ–∂–µ, –Ω–æ –ù–ï —Ç–æ—á–Ω–æ –ê–ª–¥–∞—Ä –ö–æ—Å–µ
–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: 60-70%
```

### –° –æ–±—É—á–µ–Ω–Ω–æ–π LoRA:
```
–ü—Ä–æ–º–ø—Ç: "aldar_kose_character in the steppe"
–†–µ–∑—É–ª—å—Ç–∞—Ç: –¢–û–ß–ù–û –ê–ª–¥–∞—Ä –ö–æ—Å–µ –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤
–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: 95-98%
```

## 3 –°–ø–æ—Å–æ–±–∞ –û–±—É—á–µ–Ω–∏—è LoRA

### –°–ø–æ—Å–æ–± 1: Google Colab (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø) ‚≠ê

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π GPU (Tesla T4)
- ‚úÖ –ë—ã—Å—Ç—Ä–æ (30-60 –º–∏–Ω—É—Ç)
- ‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- ‚úÖ –ù–µ –Ω–∞–≥—Ä—É–∂–∞–µ—Ç –≤–∞—à –∫–æ–º–ø—å—é—Ç–µ—Ä

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –¢—Ä–µ–±—É–µ—Ç Google –∞–∫–∫–∞—É–Ω—Ç
- ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (12 —á–∞—Å–æ–≤)

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**

1. –û—Ç–∫—Ä–æ–π—Ç–µ –≥–æ—Ç–æ–≤—ã–π Colab notebook:
   - –°–º. —Ñ–∞–π–ª `GOOGLE_COLAB_GUIDE.md`
   - –ò–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∏–∂–µ

2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ 5 —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ Colab

3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ (1 –∫–ª–∏–∫)

4. –°–∫–∞—á–∞–π—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é LoRA (~100 MB)

5. –ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤ `models/aldar_kose_lora.safetensors`

---

### –°–ø–æ—Å–æ–± 2: –õ–æ–∫–∞–ª—å–Ω–æ–µ –û–±—É—á–µ–Ω–∏–µ (train_lora_real.py)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- ‚úÖ –ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å
- ‚úÖ –ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–æ –Ω–∞ M1 (2-4 —á–∞—Å–∞)
- ‚ùå –¢—Ä–µ–±—É–µ—Ç ~10GB —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏
- ‚ùå –ù–∞–≥—Ä–µ–≤–∞–µ—Ç MacBook

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**

```bash
# 1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
pip install -r requirements.txt

# 2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
ls aldar*.png
# –î–æ–ª–∂–Ω–æ –±—ã—Ç—å: aldar1.png, aldar2.png, ..., aldar5.png

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ
python3 train_lora_real.py

# –ü—Ä–æ—Ü–µ—Å—Å:
# [1/5] –ó–∞–≥—Ä—É–∑–∫–∞ SDXL –º–æ–¥–µ–ª–∏...          (5-10 –º–∏–Ω –ø–µ—Ä–≤—ã–π —Ä–∞–∑)
# [2/5] –ê–Ω–∞–ª–∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤...             (2-3 –º–∏–Ω)
# [3/5] –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...                (1 –º–∏–Ω)
# [4/5] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...             (1 –º–∏–Ω)
# [5/5] –û–±—É—á–µ–Ω–∏–µ (100 —ç–ø–æ—Ö)...           (2-4 —á–∞—Å–∞)
#       Epoch 1/100: loss=0.5432
#       Epoch 20/100: loss=0.3214 [checkpoint saved]
#       ...
#       Epoch 100/100: loss=0.0987
# ‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!

# 4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
ls models/aldar_kose_lora.safetensors
ls static/generated/lora_test_generation.png
```

**–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è:**

–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ `train_lora_real.py`:

```python
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–±–æ–ª—å—à–µ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –¥–æ–ª—å—à–µ)
trainer.train(
    num_epochs=100,      # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ 200-500 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    learning_rate=1e-4   # –£–º–µ–Ω—å—à–∏—Ç–µ –¥–æ 5e-5 –µ—Å–ª–∏ loss –Ω–µ –ø–∞–¥–∞–µ—Ç
)
```

---

### –°–ø–æ—Å–æ–± 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ü—Ä–æ–º–ø—Ç—ã (–ë–ï–ó LoRA)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ
- ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ú–µ–Ω—å—à–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (70-80% vs 95% —Å LoRA)
- ‚ùå –ù—É–∂–Ω–æ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –ø–∏—Å–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã

**–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –£–ñ–ï –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞** –¥–ª—è —ç—Ç–æ–≥–æ —Å–ø–æ—Å–æ–±–∞:

```python
# –í config.py —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ:
TRAINING_CAPTION = (
    "2D storybook illustration of Kazakh folk hero, "
    "orange patterned chapan robe with traditional ornaments, "
    "friendly smiling expression with black mustache, "
    "small topknot hairstyle, round friendly face with narrow eyes, "
    "warm skin tone, simplified cartoon proportions"
)
```

–ü—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:
```bash
python3 app.py
# –ò–ª–∏
python3 train_lora_competitive.py
```

---

## –ü–æ–Ω–∏–º–∞–Ω–∏–µ –ü—Ä–æ—Ü–µ—Å—Å–∞ –û–±—É—á–µ–Ω–∏—è LoRA

### –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏:

```python
# 1. –ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò
SDXL = load_model("stabilityai/stable-diffusion-xl-base-1.0")
# SDXL –∑–Ω–∞–µ—Ç: –∫–∞–∫ —Ä–∏—Å–æ–≤–∞—Ç—å –ª—é–¥–µ–π, –ø–µ–π–∑–∞–∂–∏, –æ–±—ä–µ–∫—Ç—ã...
# –ù–û –ù–ï –∑–Ω–∞–µ—Ç: –∫—Ç–æ —Ç–∞–∫–æ–π –ê–ª–¥–∞—Ä –ö–æ—Å–µ

# 2. –ê–ù–ê–õ–ò–ó –†–ï–§–ï–†–ï–ù–°–û–í
for image in [aldar1, aldar2, aldar3, aldar4, aldar5]:
    features = analyze(image)
    # –ò–∑–≤–ª–µ–∫–∞–µ–º: —Ñ–æ—Ä–º–∞ –ª–∏—Ü–∞, —Ü–≤–µ—Ç –æ–¥–µ–∂–¥—ã, –ø—Ä–∏—á–µ—Å–∫–∞, —Å—Ç–∏–ª—å...

# 3. –°–û–ó–î–ê–ù–ò–ï LORA –ê–î–ê–ü–¢–ï–†–ê
lora = create_lora_adapter(rank=16)
# LoRA - —ç—Ç–æ –º–∞–ª–µ–Ω—å–∫–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (100MB) –∫–æ—Ç–æ—Ä–∞—è "–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç" SDXL

# 4. –û–ë–£–ß–ï–ù–ò–ï (100-500 —ç–ø–æ—Ö)
for epoch in range(100):
    for image in references:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º SDXL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        generated = SDXL.generate(prompt)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–º
        loss = compare(generated, image)

        # –û–±–Ω–æ–≤–ª—è–µ–º LoRA –≤–µ—Å–∞
        lora.update_weights(loss)

# 5. –†–ï–ó–£–õ–¨–¢–ê–¢
# SDXL + LoRA —Ç–µ–ø–µ—Ä—å –∑–Ω–∞—é—Ç –∫–∞–∫ —Ä–∏—Å–æ–≤–∞—Ç—å –ê–ª–¥–∞—Ä –ö–æ—Å–µ!
```

### –ö–ª—é—á–µ–≤—ã–µ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –ß—Ç–æ –¥–µ–ª–∞–µ—Ç | –ö–∞–∫ –∏–∑–º–µ–Ω—è—Ç—å |
|----------|----------|------------|--------------|
| **num_epochs** | 100-500 | –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –¥–∞–Ω–Ω—ã–º | ‚Üë = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –¥–æ–ª—å—à–µ |
| **learning_rate** | 1e-4 | –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è | ‚Üì –µ—Å–ª–∏ loss –Ω–µ –ø–∞–¥–∞–µ—Ç |
| **lora_rank** | 16 | –†–∞–∑–º–µ—Ä LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ | ‚Üë = –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π, –Ω–æ –±–æ–ª—å—à–µ —Ñ–∞–π–ª |
| **lora_alpha** | 16 | –°–∏–ª–∞ LoRA —ç—Ñ—Ñ–µ–∫—Ç–∞ | –û–±—ã—á–Ω–æ = rank |

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –û–±—É—á–µ–Ω–∏—è:

```
Epoch 1/100: loss=0.5432   ‚Üê –í—ã—Å–æ–∫–∏–π loss = –ø–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
Epoch 20/100: loss=0.3214  ‚Üê Loss –ø–∞–¥–∞–µ—Ç = –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è
Epoch 50/100: loss=0.1543  ‚Üê –•–æ—Ä–æ—à–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
Epoch 100/100: loss=0.0987 ‚Üê –ù–∏–∑–∫–∏–π loss = —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
```

**–•–æ—Ä–æ—à–∏–π loss:** < 0.15
**–û—Ç–ª–∏—á–Ω—ã–π loss:** < 0.10
**–ò–¥–µ–∞–ª—å–Ω—ã–π loss:** < 0.05

---

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –û–±—É—á–µ–Ω–Ω–æ–π LoRA

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ:

```bash
# 1. –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
python3 -c "
from local_image_generator import LocalImageGenerator
gen = LocalImageGenerator()
img = gen.generate_single('aldar_kose_character in the steppe')
img.save('test.png')
print('‚úì –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: test.png')
"

# 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏
python3 train_lora_competitive.py
# –°–∏—Å—Ç–µ–º–∞ —Å—Ä–∞–≤–Ω–∏—Ç –≤–∞—à—É LoRA —Å Freepik, DeepAI, Gemini
# –ò –ø–æ–∫–∞–∂–µ—Ç –≥–¥–µ —É–ª—É—á—à–∞—Ç—å

# 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
python3 app.py
# –û—Ç–∫—Ä–æ–π—Ç–µ http://localhost:8080
```

---

## Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: "Out of memory"

**–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç–µ batch size –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Colab

```python
# –í train_lora_real.py
TRAIN_BATCH_SIZE = 1  # –£–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π
```

### –ü—Ä–æ–±–ª–µ–º–∞: Loss –Ω–µ –ø–∞–¥–∞–µ—Ç

**–†–µ—à–µ–Ω–∏–µ 1:** –£–º–µ–Ω—å—à–∏—Ç–µ learning rate
```python
trainer.train(learning_rate=5e-5)  # –ë—ã–ª–æ 1e-4
```

**–†–µ—à–µ–Ω–∏–µ 2:** –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
```python
trainer.train(num_epochs=200)  # –ë—ã–ª–æ 100
```

### –ü—Ä–æ–±–ª–µ–º–∞: LoRA –ø–µ—Ä–µ–æ–±—É—á–∏–ª–∞—Å—å (overfitting)

**–°–∏–º–ø—Ç–æ–º—ã:** –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¢–û–ß–ù–´–ï –∫–æ–ø–∏–∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤, –Ω–æ –ø–ª–æ—Ö–æ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑—É–µ—Ç

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
trainer.train(num_epochs=50)  # –ë—ã–ª–æ 100

# –ò–ª–∏ –¥–æ–±–∞–≤—å—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
lora_config = LoraConfig(
    lora_dropout=0.2  # –ë—ã–ª–æ 0.1
)
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–∞—è

**–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç–µ inference steps
```python
# –í config.py
NUM_INFERENCE_STEPS = 20  # –ë—ã–ª–æ 30
```

---

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –¢–µ—Ö–Ω–∏–∫–∏

### 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CLIP –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

```python
from feature_analyzer import FeatureAnalyzer

analyzer = FeatureAnalyzer()
score = analyzer.calculate_similarity(generated_image, reference_images)

if score < 0.8:
    print("–ö–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∑–∫–æ–µ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
```

### 2. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π Learning Rate

```python
# –£–º–µ–Ω—å—à–∞–µ–º LR –µ—Å–ª–∏ loss –ø–µ—Ä–µ—Å—Ç–∞–ª –ø–∞–¥–∞—Ç—å
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=10
)
```

### 3. Augmentation –¥–∞–Ω–Ω—ã—Ö

```python
from torchvision import transforms

augment = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.RandomRotation(5)
])
```

---

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ü–æ–¥—Ö–æ–¥–æ–≤

| –ö—Ä–∏—Ç–µ—Ä–∏–π | Colab GPU | –õ–æ–∫–∞–ª—å–Ω–æ M1 | –¢–æ–ª—å–∫–æ –ü—Ä–æ–º–ø—Ç—ã |
|----------|-----------|-------------|----------------|
| –°–∫–æ—Ä–æ—Å—Ç—å | 30-60 –º–∏–Ω | 2-4 —á–∞—Å–∞ | –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ |
| –ö–∞—á–µ—Å—Ç–≤–æ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| –°–ª–æ–∂–Ω–æ—Å—Ç—å | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–∞—è | –ù–∏–∑–∫–∞—è |
| –°—Ç–æ–∏–º–æ—Å—Ç—å | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ |
| –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å | 95-98% | 95-98% | 70-80% |

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

**–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö –≤ ML:**
1. –ù–∞—á–Ω–∏—Ç–µ —Å–æ —Å–ø–æ—Å–æ–±–∞ 3 (–ø—Ä–æ–º–ø—Ç—ã) - –∑–∞–ø—É—Å—Ç–∏—Ç–µ `python3 train_lora_competitive.py`
2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
3. –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç - –≥–æ—Ç–æ–≤–æ!
4. –ï—Å–ª–∏ –Ω–µ—Ç - –ø–æ–ø—Ä–æ–±—É–π—Ç–µ Colab (—Å–ø–æ—Å–æ–± 1)

**–î–ª—è ML-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤:**
1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ `python3 train_lora_real.py` –ª–æ–∫–∞–ª—å–Ω–æ
2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
3. –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ loss –∏ –∫–∞—á–µ—Å—Ç–≤–æ
4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `train_lora_competitive.py` –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

---

## –°–ª–µ–¥—É—é—â–∏–µ –®–∞–≥–∏

–ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LoRA:

```bash
# 1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ LoRA —Ä–∞–±–æ—Ç–∞–µ—Ç
ls models/aldar_kose_lora.safetensors

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
python3 train_lora_competitive.py

# 3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
python3 app.py

# 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ —Å—Ç–æ—Ä–∏–±–æ—Ä–¥—ã —Å –≤–∞—à–µ–π LoRA!
```

---

**–ì–æ—Ç–æ–≤—ã –Ω–∞—á–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â–µ–µ machine learning –æ–±—É—á–µ–Ω–∏–µ?**

–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –≤—ã—à–µ –∏ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º! üöÄ
